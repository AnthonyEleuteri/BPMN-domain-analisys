{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2fdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (GPT2Tokenizer, GPT2Model, \n",
    "                          set_seed,\n",
    "                          training_args,\n",
    "                          trainer,\n",
    "                          GPT2Config,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "\n",
    "# Funzione che dato un 'path' restituisce il suo encoding.  \n",
    "def get_file_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content=f.read()\n",
    "        result = chardet.detect(content)\n",
    "        return result['encoding']\n",
    "        \n",
    "\n",
    "set_seed(123)\n",
    "epochs=4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Numero massimo della sequenza\n",
    "# La sequenza <80 avrà del padding, la sequenza >80 sarà troncata\n",
    "max_length = 510\n",
    "\n",
    "# Usiamo la cpu se la gpu non viene trova\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Nome del trasformers model pre-allenato\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dizionario delle etichette e il loro ID\n",
    "labels_ids = {'Manufacturing': 0, 'Logistics':1, 'Public Administration': 2, 'Healthcare': 3, 'Education': 4}\n",
    "\n",
    "# Numero di etichette che stiamo utilizzando\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    GPT2ForSequenceClassification\n",
    ")\n",
    "\n",
    "class BPMNDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.n_examples = len(texts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.texts[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, \n",
    "                 use_tokenizer, \n",
    "                 max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_sequence_len)\n",
    "        return inputs\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Utilizzo tqdm per visualizzare una barra di avanzamento mentre itero sui batch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        #loss=discrepanza tra le previsioni del modello e i valori reali dell'obiettivo (ground truth)\n",
    "        #logits=appresentano le \"probabilità\" che il modello assegna a ciascuna classe di output\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #Aggiorno i pesi dell'ottimizzatore e lo scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "        \n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return true_labels, predictions_labels\n",
    "\n",
    "\n",
    "def validation(dataloader, device_, model):\n",
    "    predicted_probabilities = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        batch = {k: v.to(device_) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_probabilities.extend(probabilities.tolist())\n",
    "\n",
    "    return predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf5415a-12f7-4250-8cab-9bcd3bef243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BPMNDomainDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        if not os.path.isfile(path):\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "        \n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "        self.descriptions = self.df['Labels'].to_list()\n",
    "        self.domains = self.df['CollectionName'].to_list()  \n",
    "        self.flattened_domains = [label for sublist in self.domains for label in sublist.split(',')]    \n",
    "        self.n_examples = len(self.descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\"text\": self.descriptions[item], \"label\": self.flattened_domains[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollatorDomain(object):\n",
    "    def __init__(self, \n",
    "                use_tokenizer, \n",
    "                labels_encoder, \n",
    "                max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence.get('text', None) for sequence in sequences]  # Use .get() with default None\n",
    "        labels = [sequence.get('label', None) for sequence in sequences]  # Use .get() with default None\n",
    "        label_ids = [self.labels_encoder[label] for label in labels]\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        inputs['labels'] = torch.tensor(label_ids)  \n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c031c35d-2123-4d77-9595-e3c56acdaa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [23/Mar/2024 23:09:14] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and model...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded to `cpu`\n",
      "Dealing with Train...\n",
      "Created `train_dataset` with 15 examples!\n",
      "Created `train_dataloader` with 1 batches!\n",
      "Dealing with Validation...\n",
      "Created `valid_dataset` with 21 examples!\n",
      "Created `eval_dataloader` with 1 batches!\n",
      "Epoch loop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.98it/s]\u001b[A\n",
      " 25%|█████████████████████████▊                                                                             | 1/4 [00:20<01:02, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:21<00:00, 21.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.89it/s]\u001b[A\n",
      " 50%|███████████████████████████████████████████████████▌                                                   | 2/4 [00:42<00:42, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.83s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.96it/s]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████████████████████████▎                         | 3/4 [01:03<00:21, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.79it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:24<00:00, 21.09s/it]\n",
      "127.0.0.1 - - [23/Mar/2024 23:10:47] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2024 23:11:51] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and model...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n",
      "Dealing with Train...\n",
      "Created `train_dataset` with 15 examples!\n",
      "Created `train_dataloader` with 1 batches!\n",
      "Dealing with Validation...\n",
      "Created `valid_dataset` with 21 examples!\n",
      "Created `eval_dataloader` with 1 batches!\n",
      "Epoch loop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.96it/s]\u001b[A\n",
      " 25%|█████████████████████████▊                                                                             | 1/4 [00:20<01:02, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.88it/s]\u001b[A\n",
      " 50%|███████████████████████████████████████████████████▌                                                   | 2/4 [00:41<00:41, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.33it/s]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████████████████████████▎                         | 3/4 [01:02<00:20, 20.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.80it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:23<00:00, 20.77s/it]\n",
      "127.0.0.1 - - [23/Mar/2024 23:13:21] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and model...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n",
      "Dealing with Train...\n",
      "Created `train_dataset` with 15 examples!\n",
      "Created `train_dataloader` with 1 batches!\n",
      "Dealing with Validation...\n",
      "Created `valid_dataset` with 27 examples!\n",
      "Created `eval_dataloader` with 1 batches!\n",
      "Epoch loop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.44it/s]\u001b[A\n",
      " 25%|█████████████████████████▊                                                                             | 1/4 [00:20<01:02, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.43s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.43it/s]\u001b[A\n",
      " 50%|███████████████████████████████████████████████████▌                                                   | 2/4 [00:41<00:41, 20.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.64s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.01it/s]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████████████████████████▎                         | 3/4 [01:02<00:20, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.81it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:22<00:00, 20.65s/it]\n",
      "127.0.0.1 - - [23/Mar/2024 23:17:27] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2024 23:18:13] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from collections import defaultdict\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Funzione per l'elaborazione del testo\n",
    "def elabora_testo(testo):\n",
    "\n",
    "    print('Loading configuration and model...')\n",
    "    model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                          num_labels=n_labels)\n",
    "    print('Loading tokenizer...')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    print('Loading model...')\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                                      config=model_config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    print('Model loaded to `%s`'%device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gpt2_classification_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,\n",
    "                                                          max_sequence_len=max_length)\n",
    "    gpt2_classification_collator_domain = Gpt2ClassificationCollatorDomain(use_tokenizer=tokenizer,\n",
    "                                                                       labels_encoder=labels_ids,\n",
    "                                                                       max_sequence_len=max_length)\n",
    "    print('Dealing with Train...')\n",
    "    train_dataset = BPMNDomainDataset(path='./AI_Generated_Datas/CombinedGeneratedWords.csv')\n",
    "    print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=gpt2_classification_collator_domain)  \n",
    "    print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "    print('Dealing with Validation...')\n",
    "    valid_dataset = BPMNDataset(testo)  \n",
    "    print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "    valid_dataloader = DataLoader(valid_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              collate_fn=gpt2_classification_collator)\n",
    "    print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "    print('Epoch loop ...')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print('Training on batches...')\n",
    "        train_labels, train_predict = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print('Validation on batches...')\n",
    "        valid_predict = validation(valid_dataloader, device, model)\n",
    "\n",
    "    # Mappa dei nomi delle etichette predette\n",
    "    label_names = {\n",
    "        0: 'Manufacturing',\n",
    "        1: 'Logistics',\n",
    "        2: 'Public Administration',\n",
    "        3: 'Healthcare',\n",
    "        4: 'Education'\n",
    "    }\n",
    "\n",
    "    # Inizializziamo un dizionario per accumulare i conteggi per tutte le etichette\n",
    "    counts_all = defaultdict(int)\n",
    "\n",
    "    # Iteriamo su ogni insieme di probabilità predette\n",
    "    for pred_probabilities in valid_predict:\n",
    "        for pred_label_idx, prob in enumerate(pred_probabilities):\n",
    "            pred_label = label_names[pred_label_idx]\n",
    "            counts_all[pred_label] += prob\n",
    "\n",
    "    # Calcoliamo il totale delle probabilità\n",
    "    total_probabilities = sum(sum(pred_probabilities) for pred_probabilities in valid_predict)\n",
    "\n",
    "    # Costruiamo una stringa HTML con i risultati\n",
    "    html_results = \"<h1>Risultati</h1>\"\n",
    "    html_results += \"<ul>\"\n",
    "    for label, name in label_names.items():\n",
    "        count = counts_all[name]\n",
    "        percentage = (count / total_probabilities) * 100\n",
    "        html_results += f\"<li>{name}: {percentage:.2f}%</li>\"\n",
    "    html_results += \"</ul>\"\n",
    "    return html_results\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        testo = request.form['testo']\n",
    "        risultati = elabora_testo(testo)\n",
    "        return render_template('index.html', risultati=risultati)\n",
    "    return render_template('index.html', risultati=None)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c32e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Mappa dei nomi delle etichette predette\n",
    "label_names = {\n",
    "    0: 'Manufacturing',\n",
    "    1: 'Logistics',\n",
    "    2: 'Public Administration',\n",
    "    3: 'Healthcare',\n",
    "    4: 'Education'\n",
    "}\n",
    "\n",
    "\n",
    "# Inizializziamo un dizionario per accumulare i conteggi per tutte le etichette\n",
    "counts_all = defaultdict(int)\n",
    "\n",
    "# Iteriamo su ogni insieme di probabilità predette\n",
    "for pred_probabilities in valid_predict:\n",
    "    for pred_label_idx, prob in enumerate(pred_probabilities):\n",
    "        pred_label = label_names[pred_label_idx]\n",
    "        counts_all[pred_label] += prob\n",
    "\n",
    "# Calcoliamo il totale delle probabilità\n",
    "total_probabilities = sum(sum(pred_probabilities) for pred_probabilities in valid_predict)\n",
    "\n",
    "\n",
    "print(valid_predict)\n",
    "print(\"Percentage of predictions:\")\n",
    "for label, name in label_names.items():\n",
    "    count = counts_all[name]\n",
    "    percentage = (count / total_probabilities) * 100\n",
    "    print(f\"{name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37e464-c577-4d8c-b4ca-cf8055f4ec8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
