{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6878b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import chardet\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "file_path = \"./Starting_Datas/BPMN.csv\"\n",
    "output_file_path = \"./Output_Files/BPMN_cleaned.csv\"\n",
    "\n",
    "removed_rows_file_path = \"./Discarded_Datas/BPMN_removed_short_rows.csv\"\n",
    "\n",
    "# vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(file_path)\n",
    "print(f\"L'encoding del file '{file_path}' è: {input_file_encoding}\")\n",
    "\n",
    "# Save the data of the .csv file in a Variable\n",
    "df = pd.read_csv(file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Prendo solo le utlime due colonne\n",
    "df = df.iloc[:, [0,-2]]\n",
    "\n",
    "# Elimino i duplicati\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Pulisco le colonne dai \"^^^\" se x è una stringa tolto ^^^ altrimenti ritorno x\n",
    "df['Labels'] = df['Labels'].fillna('').apply(lambda x: \n",
    "                                             x.split(\"^^^\") if isinstance(x, str) else x)\n",
    "\n",
    "# Mi salvo dominio-etichette solo per controllare i dati\n",
    "df.to_csv('./Discarded_Datas/BPMN_domains_and_labels',index=False, sep=';')\n",
    "\n",
    "# Salvo le righe con meno di 4 parole oppure con solo lettere singole o doppie in una struttura\n",
    "removed_rows_df = df[df['Labels'].apply(lambda x: \n",
    "                                        sum(1 for item in x if isinstance(item, str) and len(item) <= 2) > len(x) / 2 or len(x) < 4)]\n",
    "\n",
    "# Espressione regolare per identificare i valori che iniziano con 's' seguiti da numeri\n",
    "pattern = r'^s\\d+$'\n",
    "\n",
    "# Filtrare le righe che corrispondono al pattern\n",
    "df = df[~df['Labels'].apply(lambda x: any(re.match(pattern, item) for item in x if isinstance(item, str)))]\n",
    "\n",
    "# Salvo le righe eliminate in un .csv\n",
    "removed_rows_df.to_csv(removed_rows_file_path, index=False, sep=';')\n",
    "\n",
    "# Mi porto nel dataframe  \n",
    "df = df[~df['Labels'].apply(lambda x: \n",
    "                            sum(1 for item in x if isinstance(item, str) and len(item) <= 2) > len(x) / 2 or len(x) < 4)]\n",
    "\n",
    "# Tolgo eCH\n",
    "df = df.drop(df[df['CollectionName']== 'eCH'].index)\n",
    "\n",
    "# Salvo il file finito in un .csv\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "df.info()\n",
    "df.head(15)\n",
    "\n",
    "# Stampo le righe eliminate\n",
    "print(\"Righe rimosse:\")\n",
    "print(removed_rows_df)\n",
    "\n",
    "#print(type(df))\n",
    "#print(type(df['Labels']))\n",
    "#print(type(df['Labels'].iloc[2]))\n",
    "#print(type(df['Labels'].iloc[2][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4b6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import chardet\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_cleaned.csv'\n",
    "\n",
    "# Creazione del dataset con i suoi metodi richiesti di default\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        \n",
    "         # Prima di tutto controlliamo che il path esista\n",
    "        if not os.path.isfile(path):\n",
    "            # Se il path non è valido allora solleviamo un'eccezione\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "\n",
    "        input_file_encoding = get_file_encoding(input_file_path)\n",
    "\n",
    "        # Carico il file .csv\n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "\n",
    "        self.data = self.df['Labels']\n",
    "        self.length = len(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        # Potrei anche usare un tokenizer quì e ritornarlo sotto\n",
    "        return sample\n",
    "        \n",
    "# Creazione del dataloader che sarà necessario ai fini di usare un algoritmo per la divisione di frasi condensate\n",
    "def create_dataloader(dataframe, batch_size, shuffle=True):\n",
    "    dataset = CustomDataset(dataframe)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3283c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './Output_Files/BPMN_cleaned.csv' è: utf-8.\n",
      "L'encoding del file './Output_Files/BPMN_cleaned.csv' è: utf-8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization Progress: 100%|████████████████████████████████████████████████████████████████████████████████████| 393/393 [00:24<00:00, 16.36it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (4716) does not match length of index (12556)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 51\u001b[0m\n\u001b[1;32m     42\u001b[0m     tokenized_data\u001b[38;5;241m.\u001b[39mextend(tokenized_batch)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Itero ogni batch del dataloader e applichiamo wordninja per separare le frasi condensate\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#for batch in tqdm(dataloader, desc='Tokenization Progress'):à\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#    cleaned_batch = [remove_punctuation(label) for label in batch]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#    tokenized_batch = [wordninja.split(label) for label in cleaned_batch]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#    tokenized_data.extend(tokenized_batch)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_data\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Salvo il DataFrame risultante in un nuovo file CSV\u001b[39;00m\n\u001b[1;32m     54\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(output_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4306\u001b[0m     ):\n\u001b[1;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5039\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5039\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (4716) does not match length of index (12556)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import wordninja\n",
    "import chardet\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "##Script che separa parola per parola\n",
    "\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_cleaned.csv'\n",
    "output_file_path = './Output_Files/BPMN_uncondensed_sentences_rows.csv'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "\n",
    "# Leggo il file CSV in un DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Creazione del dataloader che creerà un dataset utile da passare al 'T5' \n",
    "dataloader = create_dataloader(input_file_path, batch_size)\n",
    "tokenized_data = []\n",
    "\n",
    "# Rimozione della punteggiatura prima della tokenizzazione\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        return text \n",
    "\n",
    "# Itero ogni batch del dataloader e applichiamo wordninja per separare le frasi condensate\n",
    "for batch in tqdm(dataloader, desc='Tokenization Progress'):\n",
    "    cleaned_batch = [remove_punctuation(label) for label in batch]\n",
    "    tokenized_batch = [wordninja.split(label) for label in cleaned_batch]\n",
    "    tokenized_data.extend(tokenized_batch)\n",
    "\n",
    "\n",
    "df['Labels'] = tokenized_data\n",
    "\n",
    "# Salvo il DataFrame risultante in un nuovo file CSV\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "#print(type(tokenized_data)) #List\n",
    "#print(type(tokenized_data[2])) #List\n",
    "#print(type(tokenized_data[2][2])) #String\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5a5ec05-a1a3-4b73-93c5-e87b1ffa43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './Output_Files/BPMN_cleaned.csv' è: utf-8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12556/12556 [00:00<00:00, 72740.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 12556/12556 [01:01<00:00, 205.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import wordninja\n",
    "import chardet\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "##Script che mi ritorna una frase unica\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_cleaned.csv'\n",
    "output_file_path = './Output_Files/BPMN_uncondensed2_sentences_rows.csv'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}.\")\n",
    "\n",
    "# Leggo il file CSV in un DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Rimozione della punteggiatura prima della tokenizzazione\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        return text \n",
    "\n",
    "# Funzione per separare le parole unite dai token\n",
    "def split_words(text):\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join(wordninja.split(text))\n",
    "    else:\n",
    "        return text \n",
    "\n",
    "# Applica la rimozione della punteggiatura e la separazione delle parole ai token nel DataFrame\n",
    "df['Labels'] = df['Labels'].progress_apply(remove_punctuation)\n",
    "df['Labels'] = df['Labels'].progress_apply(split_words)\n",
    "\n",
    "# Salvo il DataFrame risultante in un nuovo file CSV\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690b1d9a-7cc9-4800-99c3-4335ff3f541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './Output_Files/BPMN_uncondensed_sentences_rows.csv' è: ascii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering DataFrame:  92%|█████████████████████████████████████████████████████████████████████████      | 11604/12556 [1:00:03<04:55,  3.22row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sono state eliminate 952 righe.\n",
      "Righe eliminate: \n",
      "       CollectionName                                             Labels  \\\n",
      "17             BPMAI  [Act, i, vida, d, 1, Act, i, vida, d, 2, Act, ...   \n",
      "27             BPMAI  [An, r, ufer, fol, g, reich, An, ru, f, nicht,...   \n",
      "55             BPMAI  [Auto, ma, tisch, Ende, 3, Ende, 1, Ende, 2, E...   \n",
      "70             BPMAI                                                 []   \n",
      "74             BPMAI  [Ban, f, gene, hm, i, gen, Be, stellan, for, d...   \n",
      "...              ...                                                ...   \n",
      "12472        Camunda  [Computer, backend, Computer, backend, Default...   \n",
      "12493        Camunda  [Mit, g, lied, be, stehen, d, Mit, g, lied, Ne...   \n",
      "12519        Camunda  [Ly, mp, fk, not, en, status, N, 0, Lymph, kno...   \n",
      "12542        Camunda  [Ab, we, sen, he, it, G, le, it, zeit, an, tra...   \n",
      "12549        Camunda  [Ent, it, Ent, it, KO, Modifier, less, old, es...   \n",
      "\n",
      "       is_desired_language  \n",
      "17                   False  \n",
      "27                   False  \n",
      "55                   False  \n",
      "70                   False  \n",
      "74                   False  \n",
      "...                    ...  \n",
      "12472                False  \n",
      "12493                False  \n",
      "12519                False  \n",
      "12542                False  \n",
      "12549                False  \n",
      "\n",
      "[952 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import chardet\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "##LANGDETECT\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_uncondensed_sentences_rows.csv'\n",
    "output_file_path = './Output_Files/BPMN_cleaned_languages.csv'\n",
    "removed_rows_lang_file_path = \"./Discarded_Datas/BPMN_removed_lang_rows.csv\"\n",
    "\n",
    "# Vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}.\")\n",
    "\n",
    "# Leggo il file CSV in un DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Imposto la lingua che vogliamo mantenere e ritorno se la stringa appartiene \n",
    "def is_desired_language(text):\n",
    "    try:\n",
    "        return detect(text)=='en'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "# Metodo che cicla ogni etichetta\n",
    "def is_desired_language_list(labels):\n",
    "    return any(is_desired_language(str(label)) for label in labels)\n",
    "\n",
    "def filter_dataframe(df):\n",
    "    # Wrapping the DataFrame filtering process with tqdm\n",
    "    with tqdm(total=len(df), desc='Filtering DataFrame', unit='row') as pbar:\n",
    "        df['Labels'] = df['Labels'].apply(ast.literal_eval)\n",
    "        df['is_desired_language'] = df['Labels'].apply(is_desired_language_list)\n",
    "        \n",
    "        discarded_rows = df[~df['is_desired_language']]\n",
    "        filtered_df = df[df['is_desired_language']].drop(['is_desired_language'], axis=1)\n",
    "\n",
    "        # Mi salvo le righe eliminate perchè non consone ad 'en'\n",
    "        discarded_rows.to_csv(removed_rows_lang_file_path, index=False, sep=';')\n",
    "\n",
    "        # stampo le righe eliminate\n",
    "        print(f\"Sono state eliminate {len(discarded_rows)} righe.\")\n",
    "        print(f\"Righe eliminate: \\n {discarded_rows}\")\n",
    "        # Update progress bar\n",
    "        pbar.update(len(df) - len(discarded_rows))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Applico il filtro al DataFrame\n",
    "filtered_df = filter_dataframe(df)\n",
    "\n",
    "# Salvo il DataFrame risultante in un nuovo file CSV\n",
    "filtered_df.to_csv(output_file_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0670717f-0d0e-4504-b1a2-5997038d9138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering DataFrame:  39%|███████████████████████████████▊                                                 | 4924/12556 [00:20<00:31, 243.21row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sono state eliminate 7632 righe.\n",
      "Righe eliminate: \n",
      "       CollectionName                                             Labels  \\\n",
      "0              BPMAI  A rz t unter such t nach Vol l st n dig ke it ...   \n",
      "3              BPMAI  A NO A NO Default Input Set Default Input Set ...   \n",
      "4              BPMAI  Date n be re its vor hand en J a J a J a Kon t...   \n",
      "8              BPMAI  Analysed ela demand e Cr ation du com pte Est ...   \n",
      "9              BPMAI  Kun de Kun de Kun de Kun de Kun de Log is tik ...   \n",
      "...              ...                                                ...   \n",
      "12551        Camunda  Ange bota us w hlen Ange botS 1 e in hole n An...   \n",
      "12552        Camunda  j a ne in Regress mg lich Regress pr fen Regre...   \n",
      "12553        Camunda  3 Ange bot ee in hole n Klein men ge Lager arb...   \n",
      "12554        Camunda  Bon it t be iSch ufa an fragen Er geb nisan Ba...   \n",
      "12555        Camunda  ander Rei he An ges tell ter An ges tell ter B...   \n",
      "\n",
      "       is_desired_language  \n",
      "0                    False  \n",
      "3                    False  \n",
      "4                    False  \n",
      "8                    False  \n",
      "9                    False  \n",
      "...                    ...  \n",
      "12551                False  \n",
      "12552                False  \n",
      "12553                False  \n",
      "12554                False  \n",
      "12555                False  \n",
      "\n",
      "[7632 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import langid\n",
    "from tqdm import tqdm\n",
    "\n",
    "##LANGID\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_uncondensed2_sentences_rows.csv'\n",
    "output_file_path = './Output_Files/BPMN_cleaned2_languages.csv'\n",
    "removed_rows_lang_file_path = \"./Discarded_Datas/BPMN_removed2_lang_rows.csv\"\n",
    "\n",
    "# Funzione per controllare la lingua di un testo\n",
    "def is_desired_language(text):\n",
    "    try:\n",
    "        lang, _ = langid.classify(text)\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def filter_dataframe(df):\n",
    "    # Wrapping the DataFrame filtering process with tqdm\n",
    "    with tqdm(total=len(df), desc='Filtering DataFrame', unit='row') as pbar:\n",
    "        # Applica la funzione is_desired_language a ogni testo del DataFrame\n",
    "        df['is_desired_language'] = df['Labels'].apply(is_desired_language)\n",
    "        \n",
    "        # Seleziona le righe del DataFrame in cui il testo è in inglese\n",
    "        filtered_df = df[df['is_desired_language']]\n",
    "        # Salva le righe scartate in un file separato\n",
    "        discarded_rows = df[~df['is_desired_language']]\n",
    "        discarded_rows.to_csv(removed_rows_lang_file_path, index=False, sep=';')\n",
    "\n",
    "        # Stampa le righe eliminate\n",
    "        print(f\"Sono state eliminate {len(discarded_rows)} righe.\")\n",
    "        print(f\"Righe eliminate: \\n {discarded_rows}\")\n",
    "        \n",
    "        # Aggiorna la barra di avanzamento\n",
    "        pbar.update(len(df) - len(discarded_rows))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Leggi il file CSV in un DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python')\n",
    "\n",
    "# Applica il filtro al DataFrame\n",
    "filtered_df = filter_dataframe(df)\n",
    "\n",
    "# Salva il DataFrame risultante in un nuovo file CSV\n",
    "filtered_df.to_csv(output_file_path, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d74d6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './Output_Files/BPMN_cleaned2_languages.csv' è: ascii\n",
      "Numero di diversi domini nel testing_data:\n",
      "CollectionName\n",
      "BPMAI      1225\n",
      "Camunda     103\n",
      "Name: count, dtype: int64\n",
      "     CollectionName                                             Labels  \\\n",
      "361           BPMAI  810 min Add pasta Boil water with little salt ...   \n",
      "1841          BPMAI  Check internet connect tion Client Client Conn...   \n",
      "402           BPMAI                        Pool 1 Pool 1 Pool 2 Pool 2   \n",
      "4066          BPMAI  Customer WS Customer WS Customer WS Customer W...   \n",
      "3012          BPMAI  200 am Backup database Shrink the database bac...   \n",
      "\n",
      "      is_desired_language  \n",
      "361                  True  \n",
      "1841                 True  \n",
      "402                  True  \n",
      "4066                 True  \n",
      "3012                 True  \n",
      "\n",
      "Numero di diversi domini nel training_data:\n",
      "CollectionName\n",
      "BPMAI      3315\n",
      "Camunda     281\n",
      "Name: count, dtype: int64\n",
      "     CollectionName                                             Labels  \\\n",
      "1220          BPMAI  3 advise collection agency available Check cus...   \n",
      "1692          BPMAI  Accept term and Condition Add user details to ...   \n",
      "3098          BPMAI  Perform Quality Inspection Material OK problem...   \n",
      "3928          BPMAI  attraction booking attraction booking succeede...   \n",
      "4111          BPMAI           end start task test process test process   \n",
      "\n",
      "      is_desired_language  \n",
      "1220                 True  \n",
      "1692                 True  \n",
      "3098                 True  \n",
      "3928                 True  \n",
      "4111                 True  \n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "#SPLIT IN TRAIN AND TEST\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './Output_Files/BPMN_cleaned2_languages.csv'\n",
    "output_training_file = './Training_Testing_Files/BPMN_training2.csv'\n",
    "output_testing_file = './Training_Testing_Files/BPMN_testing2.csv'\n",
    "\n",
    "# vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}\")\n",
    "\n",
    "# Loading the DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Funzione che divide il DataFrame in proporzion\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "# Funzione che da un dataframe ritorna un dataframe di dataframe divisi\n",
    "def split_by_collectionname(data):\n",
    "    divided_data = {}\n",
    "    for collectionname in data['CollectionName'].unique():\n",
    "        divided_data[collectionname] = data[data['CollectionName'] == collectionname].copy()\n",
    "    return divided_data\n",
    "\n",
    "# Funzione che per ogni chiave (collectionname) divide in train e testing\n",
    "def creating_train_test_dataframe(data, test_ratio):\n",
    "    training_df = []\n",
    "    testing_df = []\n",
    "    # Dividi il DataFrame per CollectionName\n",
    "    divided_data = split_by_collectionname(data)\n",
    "    # Per ogni CollectionName, dividi il DataFrame in train e test\n",
    "    for collectionname, data in divided_data.items():\n",
    "        train_df, test_df = split_train_test(data, test_ratio)\n",
    "        training_df.append(train_df)\n",
    "        testing_df.append(test_df)\n",
    "        \n",
    "        training_data=pd.concat(training_df)\n",
    "        testing_data=pd.concat(testing_df) \n",
    "    \n",
    "    return training_data, testing_data  \n",
    "        \n",
    "# Prende randomicamente e in percentuale le righe del df per training e testing\n",
    "training_data, testing_data = creating_train_test_dataframe(df, 0.27)\n",
    "\n",
    "\n",
    "# Stampo le prime 20 righe di training e testing\n",
    "# Stampa il numero dei diversi domini nel DataFrame di testing\n",
    "print(\"Numero di diversi domini nel testing_data:\")\n",
    "print(testing_data['CollectionName'].value_counts())\n",
    "print(testing_data.head())\n",
    "\n",
    "# Stampa il numero dei diversi domini nel DataFrame di training\n",
    "print(\"\\nNumero di diversi domini nel training_data:\")\n",
    "print(training_data['CollectionName'].value_counts())\n",
    "print(training_data.head())\n",
    "\n",
    "# salva in file rispettivi per training e testing\n",
    "training_data.to_csv(output_training_file, index=False, sep=';')\n",
    "testing_data.to_csv(output_testing_file, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c5bd6-771f-4f16-b864-6ce35d2ba04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
