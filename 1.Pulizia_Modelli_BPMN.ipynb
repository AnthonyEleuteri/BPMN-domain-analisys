{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6878b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './BPMN.csv' è: utf-8\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13348 entries, 0 to 22575\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   CollectionName  13348 non-null  object\n",
      " 1   Labels          13348 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 312.8+ KB\n",
      "Righe rimosse:\n",
      "      CollectionName                                             Labels\n",
      "6                BIT                             [s00000520, s00000527]\n",
      "10               BIT                  [s00000642, s00000643, s00000645]\n",
      "11               BIT                                        [s00000026]\n",
      "12               BIT                             [s00000527, s00000671]\n",
      "16               BIT                                        [s00000716]\n",
      "...              ...                                                ...\n",
      "18836          BPMAI  [A, AC, AC, AG, AG, AJ, AJ, AK, AK, AM, AM, AP...\n",
      "19427        Camunda  [Vorgangschliessen, Zahlungbuchen, Zahlungeing...\n",
      "21581        Camunda                         [Warenversanderforderlich]\n",
      "21629        Camunda                       [dadsad, saffdsaf, saffdsaf]\n",
      "22386        Camunda                                     [1.Bed, ?, lo]\n",
      "\n",
      "[4899 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import chardet\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "file_path = \"./BPMN.csv\"\n",
    "output_file_path = \"./BPMNcleaned.csv\"\n",
    "removed_rows_file_path = \"./BPMN_removed_short_rows.csv\"\n",
    "\n",
    "# vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(file_path)\n",
    "print(f\"L'encoding del file '{file_path}' è: {input_file_encoding}\")\n",
    "\n",
    "# Save the data of the .csv file in a Variable\n",
    "df = pd.read_csv(file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Prendo solo le utlime due colonne\n",
    "df = df.iloc[:, [0,-2]]\n",
    "\n",
    "# Elimino i duplicati\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Pulisco le colonne dai \"^^^\" se x è una stringa tolto ^^^ altrimenti ritorno x\n",
    "df['Labels'] = df['Labels'].fillna('').apply(lambda x: \n",
    "                                             x.split(\"^^^\") if isinstance(x, str) else x)\n",
    "\n",
    "# Salvo le righe con meno di 4 parole oppure con solo lettere singole o doppie in una struttura\n",
    "removed_rows_df = df[df['Labels'].apply(lambda x: \n",
    "                                        sum(1 for item in x if isinstance(item, str) and len(item) <= 2) > len(x) / 2 or len(x) < 4)]\n",
    "\n",
    "# Salvo le righe eliminate in un .csv\n",
    "removed_rows_df.to_csv(removed_rows_file_path, index=False, sep=';')\n",
    "\n",
    "# Mi porto nel databse \n",
    "df = df[~df['Labels'].apply(lambda x: \n",
    "                            sum(1 for item in x if isinstance(item, str) and len(item) <= 2) > len(x) / 2 or len(x) < 4)]\n",
    "\n",
    "\n",
    "\n",
    "# Save the file .csv\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "df.info()\n",
    "df.head(15)\n",
    "\n",
    "# Print removed rows\n",
    "print(\"Righe rimosse:\")\n",
    "print(removed_rows_df)\n",
    "\n",
    "#print(type(df))\n",
    "#print(type(df['Labels']))\n",
    "#print(type(df['Labels'].iloc[2]))\n",
    "#print(type(df['Labels'].iloc[2][2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4b6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './BPMNcleaned.csv' è: utf-8.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import chardet\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './BPMNcleaned.csv'\n",
    "output_file_path = './BPMNcleanedlanguages.csv'\n",
    "\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}.\")\n",
    "\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Creazione del dataset con i suoi metodi richiesti\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        \n",
    "         # Prima di tutto controlliamo che il path esista\n",
    "        if not os.path.isfile(path):\n",
    "            # Se il path non è valido allora solleviamo un'eccezione\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "        \n",
    "        # Carico il file .csv\n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "\n",
    "        self.data = df['Labels']\n",
    "        self.length = len(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        return sample\n",
    "# Creazione del dataloader che sarà necessario ai fini di usare 'T5'\n",
    "def create_dataloader(dataframe, batch_size, shuffle=True):\n",
    "    dataset = CustomDataset(dataframe)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3283c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'encoding del file './BPMNcleaned.csv' è: utf-8.\n",
      "Sono state eliminate 5370 righe.\n",
      "Righe eliminate: \n",
      "       CollectionName                                             Labels  \\\n",
      "675            BPMAI  [ArztuntersuchtnachVollst?хndigkeit, Gewebewir...   \n",
      "678            BPMAI  [ANO, ANO, DefaultInputSet, DefaultInputSet, D...   \n",
      "679            BPMAI  [Datenbereitsvorhanden?, Ja, Ja, Ja, Kontendat...   \n",
      "687            BPMAI     [subprocessname, subprocessname, task1, task2]   \n",
      "691            BPMAI  [Auftragsbest?хtigungerstellen, Bestelldatenau...   \n",
      "...              ...                                                ...   \n",
      "13341            eCH  [52PatentpflichtAnlassabkl?хren, Bewilligunger...   \n",
      "13342            eCH  [52PatentpflichtAnlassabkl?хren, AusnahmenzurA...   \n",
      "13344            eCH  [Antragauf?ҐnderungVernehmlassungsweglangstatt...   \n",
      "13345            eCH  [Abteilungsleiter, allesi.O., allesi.O., Anpas...   \n",
      "13347            eCH  [BesprechungmitBeteiligten, Bewilligungerteile...   \n",
      "\n",
      "       is_desired_language  \n",
      "675                  False  \n",
      "678                  False  \n",
      "679                  False  \n",
      "687                  False  \n",
      "691                  False  \n",
      "...                    ...  \n",
      "13341                False  \n",
      "13342                False  \n",
      "13344                False  \n",
      "13345                False  \n",
      "13347                False  \n",
      "\n",
      "[5370 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import chardet\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './BPMNcleaned.csv'\n",
    "output_file_path = './BPMNcleanedlanguages.csv'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# Vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}.\")\n",
    "\n",
    "# Leggo il file CSV in un DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Funzione per la pulizia delle etichette con T5 utilizzando il batch processing\n",
    "def clean_labels_with_t5_batch(labels_batch):\n",
    "    input_ids = tokenizer.batch_encode_plus(labels_batch, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)['input_ids']\n",
    "    output_ids = model.generate(input_ids=input_ids, max_length=512)\n",
    "    cleaned_labels_batch = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return cleaned_labels_batch\n",
    "\n",
    "\n",
    "# Imposto la lingua che vogliamo mantenere e ritorno se la stringa appartiene \n",
    "def is_desired_language(text):\n",
    "    try:\n",
    "        return detect(text)=='en'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "# Metodo che cicla ogni etichetta\n",
    "def is_desired_language_list(labels):\n",
    "    return any(is_desired_language(str(label)) for label in labels)\n",
    "\n",
    "# Filtro ogni riga del dataframe e successivamente pulisco le righe non consone\n",
    "def filter_dataframe(df):\n",
    "    df['Labels'] = df['Labels'].apply(lambda labels: literal_eval(labels))\n",
    "    df['is_desired_language'] = df['Labels'].apply(is_desired_language_list)\n",
    "        \n",
    "    discarded_rows = df[~df['is_desired_language']]\n",
    "    filtered_df = df[df['is_desired_language']].drop(['is_desired_language'], axis=1)\n",
    "    \n",
    "    # stampo le righe eliminate\n",
    "    print(f\"Sono state eliminate {len(discarded_rows)} righe.\")\n",
    "    print(f\"Righe eliminate: \\n {discarded_rows}\")\n",
    "    #print(discarded_rows)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Creazione del dataloader che creerà un dataset utile da passare al 'T5' \n",
    "dataloader = create_dataloader(input_file_path, batch_size)\n",
    "# Itero ogni batch del dataloader e applichiamo il 'T5'\n",
    "for batch in dataloader:\n",
    "    clean_labels_with_t5_batch(batch)\n",
    "\n",
    "# Applico il filtro al DataFrame\n",
    "filtered_df = filter_dataframe(df)\n",
    "\n",
    "# Salvo il DataFrame risultante in un nuovo file CSV\n",
    "filtered_df.to_csv(output_file_path, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "input_file_path = './BPMNcleanedlanguages.csv'\n",
    "output_training_file = './BPMNtraining.csv'\n",
    "output_testing_file = './BPMNtesting.csv'\n",
    "\n",
    "# vedo l'encoding del file\n",
    "input_file_encoding = get_file_encoding(input_file_path)\n",
    "print(f\"L'encoding del file '{input_file_path}' è: {input_file_encoding}\")\n",
    "\n",
    "# Loading the DataFrame\n",
    "df = pd.read_csv(input_file_path, sep=';', engine='python', encoding=input_file_encoding)\n",
    "\n",
    "# Funzione che divide il DataFrame in proporzion\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "# Funzione che da un dataframe ritorna un dataframe di dataframe divisi\n",
    "def split_by_collectionname(data):\n",
    "    divided_data = {}\n",
    "    for collectionname in data['CollectionName'].unique():\n",
    "        divided_data[collectionname] = data[data['CollectionName'] == collectionname].copy()\n",
    "    return divided_data\n",
    "\n",
    "# Funzione che per ogni chiave (collectionname) divide in train e testing\n",
    "def creating_train_test_dataframe(data, test_ratio):\n",
    "    training_df = []\n",
    "    testing_df = []\n",
    "    # Dividi il DataFrame per CollectionName\n",
    "    divided_data = split_by_collectionname(data)\n",
    "    # Per ogni CollectionName, dividi il DataFrame in train e test\n",
    "    for collectionname, data in divided_data.items():\n",
    "        train_df, test_df = split_train_test(data, test_ratio)\n",
    "        training_df.append(train_df)\n",
    "        testing_df.append(test_df)\n",
    "        \n",
    "        training_data=pd.concat(training_df)\n",
    "        testing_data=pd.concat(testing_df) \n",
    "    \n",
    "    return training_data, testing_data  \n",
    "        \n",
    "# Prende randomicamente e in percentuale le righe del df per training e testing\n",
    "# training_data, testing_data = split_train_test(df, 0.3)\n",
    "training_data, testing_data = creating_train_test_dataframe(df, 0.3)\n",
    "\n",
    "\n",
    "# Stampo le prime 20 righe di training e testing\n",
    "# Stampa il numero dei diversi domini nel DataFrame di testing\n",
    "print(\"Numero di diversi domini nel testing_data:\")\n",
    "print(testing_data['CollectionName'].value_counts())\n",
    "print(testing_data.head())\n",
    "\n",
    "# Stampa il numero dei diversi domini nel DataFrame di training\n",
    "print(\"\\nNumero di diversi domini nel training_data:\")\n",
    "print(training_data['CollectionName'].value_counts())\n",
    "print(training_data.head())\n",
    "\n",
    "# salva in file rispettivi per training e testing\n",
    "training_data.to_csv(output_training_file, index=False, sep=';')\n",
    "testing_data.to_csv(output_testing_file, index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
