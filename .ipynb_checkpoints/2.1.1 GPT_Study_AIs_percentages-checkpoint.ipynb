{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b2fdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (GPT2Tokenizer, GPT2Model, \n",
    "                          set_seed,\n",
    "                          training_args,\n",
    "                          trainer,\n",
    "                          GPT2Config,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "set_seed(123)\n",
    "epochs=4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Numero massimo della sequenza\n",
    "# La sequenza <80 avrà del padding, la sequenza >80 sarà troncata\n",
    "max_length = 510\n",
    "\n",
    "# Usiamo la cpu se la gpu non viene trova\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Nome del trasformers model pre-allenato\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dizionario delle etichette e il loro ID\n",
    "labels_ids = {'Manufacturing': 0, 'Logistics':1, 'Public Administration': 2, 'Healthcare': 3, 'Education': 4}\n",
    "\n",
    "# Numero di etichette che stiamo utilizzando\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "097040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    GPT2ForSequenceClassification\n",
    ")\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "# Definizioni delle classi Dataset\n",
    "class BPMNDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        if not os.path.isfile(path):\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "        \n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "        self.descriptions = self.df['Labels'].to_list()\n",
    "        self.n_examples = len(self.descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.descriptions[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, \n",
    "                 use_tokenizer, \n",
    "                 max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_sequence_len)\n",
    "        return inputs\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Utilizzo tqdm per visualizzare una barra di avanzamento mentre itero sui batch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        #loss=discrepanza tra le previsioni del modello e i valori reali dell'obiettivo (ground truth)\n",
    "        #logits=appresentano le \"probabilità\" che il modello assegna a ciascuna classe di output\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #Aggiorno i pesi dell'ottimizzatore e lo scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "        \n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return true_labels, predictions_labels\n",
    "\n",
    "\n",
    "def validation(dataloader, device_, model):\n",
    "    predicted_probabilities = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        batch = {k: v.to(device_) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_probabilities.extend(probabilities.tolist())\n",
    "\n",
    "    return predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bf5415a-12f7-4250-8cab-9bcd3bef243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BPMNDomainDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        if not os.path.isfile(path):\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "        \n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "        self.descriptions = self.df['Labels'].to_list()\n",
    "        self.domains = self.df['CollectionName'].to_list()  \n",
    "        self.flattened_domains = [label for sublist in self.domains for label in sublist.split(',')]    \n",
    "        self.n_examples = len(self.descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\"text\": self.descriptions[item], \"label\": self.flattened_domains[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollatorDomain(object):\n",
    "    def __init__(self, \n",
    "                use_tokenizer, \n",
    "                labels_encoder, \n",
    "                max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence.get('text', None) for sequence in sequences]  # Use .get() with default None\n",
    "        labels = [sequence.get('label', None) for sequence in sequences]  # Use .get() with default None\n",
    "        label_ids = [self.labels_encoder[label] for label in labels]\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        inputs['labels'] = torch.tensor(label_ids)  \n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97e8cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and model...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "\n",
    "# Configuriamo il modello\n",
    "print('Loading configuration and model...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                          num_labels=n_labels)\n",
    "\n",
    "# Configuriamo il tokenizer del modello\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Impostiamo il pad token nel modello\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Carichiamo il modello\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                                      config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Carichiamo il modello su GPU o CPU\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "842fa0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n",
      "Created `train_dataset` with 5 examples!\n",
      "Created `train_dataloader` with 1 batches!\n",
      "Dealing with Validation...\n",
      "Created `valid_dataset` with 1 examples!\n",
      "Created `eval_dataloader` with 1 batches!\n"
     ]
    }
   ],
   "source": [
    "# Creo un data collator per codificare testo ed etichette in numeri\n",
    "gpt2_classification_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,\n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "gpt2_classification_collator_domain = Gpt2ClassificationCollatorDomain(use_tokenizer=tokenizer,\n",
    "                                                                       labels_encoder=labels_ids,\n",
    "                                                                       max_sequence_len=max_length)\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Creo un dataset pytorch per l'allenamento\n",
    "train_dataset = BPMNDomainDataset(path='./AI_Generated_Datas/CombinedGeneratedWords.csv')\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Carico il dataset pytorch nel dataloader per l'allenamento\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=gpt2_classification_collator_domain)  \n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Creo un dataset pytorch per la validazione\n",
    "valid_dataset = BPMNDataset(path='./ASTRAZENECA.csv')  \n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Carico il dataset pytorch nel dataloader per la validazione\n",
    "valid_dataloader = DataLoader(valid_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              collate_fn=gpt2_classification_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cde5c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.37it/s]\u001b[A\n",
      " 25%|███████████████████████████▊                                                                                   | 1/4 [00:04<00:13,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.56it/s]\u001b[A\n",
      " 50%|███████████████████████████████████████████████████████▌                                                       | 2/4 [00:08<00:08,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.79it/s]\u001b[A\n",
      " 75%|███████████████████████████████████████████████████████████████████████████████████▎                           | 3/4 [00:13<00:04,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:17<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definizione dell'ottimizzatore\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Calcolo del numero totale di passaggi di addestramento\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Creazione del learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Loop per ogni epoca\n",
    "print('Epoch loop ...')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print('Training on batches...')\n",
    "    # Eseguire l'addestramento del modello\n",
    "    train_labels, train_predict = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "\n",
    "    print('Validation on batches...')\n",
    "    # Eseguire la validazione del modello\n",
    "    valid_predict = validation(valid_dataloader, device, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9c32e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2654106020927429, 0.24662083387374878, 0.09147028625011444, 0.10680435597896576, 0.2896939516067505]]\n",
      "Percentage of predictions:\n",
      "Manufacturing: 26.54%\n",
      "Logistics: 24.66%\n",
      "Public Administration: 9.15%\n",
      "Healthcare: 10.68%\n",
      "Education: 28.97%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Mappa dei nomi delle etichette predette\n",
    "label_names = {\n",
    "    0: 'Manufacturing',\n",
    "    1: 'Logistics',\n",
    "    2: 'Public Administration',\n",
    "    3: 'Healthcare',\n",
    "    4: 'Education'\n",
    "}\n",
    "\n",
    "\n",
    "# Inizializziamo un dizionario per accumulare i conteggi per tutte le etichette\n",
    "counts_all = defaultdict(int)\n",
    "\n",
    "# Iteriamo su ogni insieme di probabilità predette\n",
    "for pred_probabilities in valid_predict:\n",
    "    for pred_label_idx, prob in enumerate(pred_probabilities):\n",
    "        pred_label = label_names[pred_label_idx]\n",
    "        counts_all[pred_label] += prob\n",
    "\n",
    "# Calcoliamo il totale delle probabilità\n",
    "total_probabilities = sum(sum(pred_probabilities) for pred_probabilities in valid_predict)\n",
    "\n",
    "\n",
    "print(valid_predict)\n",
    "print(\"Percentage of predictions:\")\n",
    "for label, name in label_names.items():\n",
    "    count = counts_all[name]\n",
    "    percentage = (count / total_probabilities) * 100\n",
    "    print(f\"{name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37e464-c577-4d8c-b4ca-cf8055f4ec8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
