{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2fdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (GPT2Tokenizer, GPT2Model, \n",
    "                          set_seed,\n",
    "                          training_args,\n",
    "                          trainer,\n",
    "                          GPT2Config,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "set_seed(123)\n",
    "epochs=4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Numero massimo della sequenza\n",
    "# La sequenza <80 avrà del padding, la sequenza >80 sarà troncata\n",
    "max_length = 510\n",
    "\n",
    "# Usiamo la cpu se la gpu non viene trova\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Nome del trasformers model pre-allenato\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dizionario delle etichette e il loro ID\n",
    "labels_ids = {'Manufacturing': 0, 'Logistics':1, 'Public Administration': 2, 'Healthcare': 3, 'Education': 4}\n",
    "\n",
    "# Numero di etichette che stiamo utilizzando\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%run \"./support_functions.ipynb\"\n",
    "\n",
    "\n",
    "#Creazione di un Dataset pytorch per il caricamento dei dati multidimensionali\n",
    "class BPMNDomainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Prima di tutto controlliamo che il path esista\n",
    "        if not os.path.isfile(path):\n",
    "            # Se il path non è valido allora solleviamo un'eccezione\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a file')\n",
    "        \n",
    "        # Carico il file .csv\n",
    "        self.df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "        self.descriptions = self.df['Labels'].to_list()\n",
    "        self.n_examples = len(self.descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.descriptions[item]}\n",
    "\n",
    "#Prepara i dati in modo che possano essere utilizzati efficacemente \n",
    "#durante l'addestramento di un modello GPT-2 per la classificazione di sequenze.\n",
    "class Gpt2ClassificationCollator(object):\n",
    "\n",
    "    def __init__(self, use_tokenizer, max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        \n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_sequence_len)\n",
    "        return inputs\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Trasferisci il batch su GPU, se disponibile\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Aggiungi le predizioni e le etichette vere\n",
    "        true_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
    "        predictions_labels.extend(logits.argmax(axis=-1).cpu().numpy().tolist())\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "def validation(dataloader, device_, model):\n",
    "    predictions_labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            # Trasferisci il batch su GPU, se disponibile\n",
    "            batch = {k: v.to(device_) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions_labels.extend(logits.argmax(axis=-1).cpu().numpy().tolist())\n",
    "            \n",
    "    return predictions_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e8cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "\n",
    "# Configuriamo il modello\n",
    "print('Loading configuration and model...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                          num_labels=n_labels)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Impostiamo il pad token nel modello\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Carichiamo il modello\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                                      config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Carichiamo il modello su GPU o CPU\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842fa0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n",
      "Created `train_dataset` with 15 examples!\n",
      "Created `train_dataloader` with 1 batches!\n",
      "Dealing with Validation...\n",
      "Created `valid_dataset` with 3504 examples!\n",
      "Created `eval_dataloader` with 110 batches!\n"
     ]
    }
   ],
   "source": [
    "# Creo un data collator per codificare testo ed etichette in numeri\n",
    "gpt2_classification_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,  \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Creo un dataset pytorch per l'allenamento\n",
    "train_dataset = BPMNDomainDataset(path='./BPMB-Labels-by-AIs.csv')\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Carico il dataset pytorch nel dataloader per l'allenamento\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=gpt2_classification_collator,\n",
    "                              num_workers=10)  # Imposta il numero di worker per il caricamento parallelo\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Creo un dataset pytorch per la validazione\n",
    "valid_dataset = BPMNDomainDataset(path='./BPMN_cleaned_languages.csv')\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Carico il dataset pytorch nel dataloader per la validazione\n",
    "valid_dataloader = DataLoader(valid_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              collate_fn=gpt2_classification_collator,\n",
    "                              num_workers=10)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/anthonyeleuteri/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anthonyeleuteri/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'BPMNDomainDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# ADAM = ADAPTIVE MOMENT ESTIMATION\n",
    "# L'AdamW è un ottimizzatore che aggiorna il modello in conseguenza della funzione \n",
    "# di perdita, in un 'Gradient descent', ad ogni epoche il loss dovrebbe scendere e trovare un MINIMO\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "# Il numero totale di total_steps è uguale a batch_size*epochs e\n",
    "# train_dataloader contiene i dati già batched\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Creiamo un learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,  # Default value in run_glue.py\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Loop per ogni epoca\n",
    "print('Epoch loop ...')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print('Training on batches...')\n",
    "    # Facciamo un'intera addestramento sul training set\n",
    "    train(model, train_dataloader, optimizer, scheduler, device)\n",
    "\n",
    "    # Facciamo un'intera validazione sul validation set\n",
    "    print('Validation on batches...')\n",
    "    predictions_labels = validation(valid_dataloader, device, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828d1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [15:27<00:00,  8.43s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Crea un dataframe con le etichette vere e predette\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df_correlation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Labels\u001b[39m\u001b[38;5;124m'\u001b[39m: true_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Labels\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_labels})\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calcola la matrice di correlazione\u001b[39;00m\n\u001b[1;32m     20\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m df_correlation\u001b[38;5;241m.\u001b[39mcorr()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"./BPMN_cleaned_languages.csv\"\n",
    "\n",
    "# Carica il file con il dataframe\n",
    "df = pd.read_csv(path, sep=';', engine='python', encoding=get_file_encoding(path))\n",
    "\n",
    "# Ottieni le etichette vere e predette\n",
    "true_labels = df['CollectionName']\n",
    "predictions_labels = validation(valid_dataloader, device, model)\n",
    "\n",
    "# Estrai il dominio interno dal DataFrame\n",
    "internal_domains = df['InternalDomain']\n",
    "\n",
    "# Costruisci un nuovo DataFrame con dominio interno e etichetta predetta\n",
    "df_correlation = pd.DataFrame({'Internal Domain': internal_domains, 'Predicted Labels': predictions_labels})\n",
    "\n",
    "# Calcola la correlazione tra il dominio interno e le etichette predette\n",
    "correlation_matrix = df_correlation.corr()\n",
    "\n",
    "# Visualizza la matrice di correlazione\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix between Internal Domain and Predicted Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c32e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
